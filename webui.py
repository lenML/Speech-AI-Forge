import os
import logging

from numpy import clip

logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)


import gradio as gr
import io
import re
import numpy as np

import torch

from modules.ssml import parse_ssml
from modules.SynthesizeSegments import SynthesizeSegments, combine_audio_segments
from modules.generate_audio import generate_audio, generate_audio_batch

from modules.speaker import speaker_mgr
from modules.data import styles_mgr

from modules.api.utils import calc_spk_style

from modules.normalization import text_normalize
from modules import refiner, config


torch._dynamo.config.cache_size_limit = 64
torch._dynamo.config.suppress_errors = True
torch.set_float32_matmul_precision("high")


def get_speakers():
    return speaker_mgr.list_speakers()


def get_styles():
    return styles_mgr.list_items()


@torch.inference_mode()
def synthesize_ssml(ssml: str, batch_size=8):
    try:
        batch_size = int(batch_size)
    except Exception:
        batch_size = 8

    # åªæ”¯æŒå•ä¸ª2000å­—ä»¥å†…çš„æ–‡æœ¬
    ssml = ssml.strip()[0:2000]

    segments = parse_ssml(ssml)

    synthesize = SynthesizeSegments(batch_size=batch_size)
    audio_segments = synthesize.synthesize_segments(segments)
    combined_audio = combine_audio_segments(audio_segments)

    buffer = io.BytesIO()
    combined_audio.export(buffer, format="wav")

    buffer.seek(0)

    return buffer.read()


@torch.inference_mode()
def tts_generate_batch_1(
    text,
    temperature,
    top_p,
    top_k,
    spk,
    infer_seed,
    use_decoder,
    prompt1,
    prompt2,
    prefix,
    style,
    disable_normalize=False,
    batch_size=8,
):
    if style == "*auto":
        style = None

    if isinstance(top_k, float):
        top_k = int(top_k)

    params = calc_spk_style(spk=spk, style=style)

    spk = params.get("spk", spk)
    infer_seed = infer_seed or params.get("seed", infer_seed)
    temperature = temperature or params.get("temperature", temperature)
    prefix = prefix or params.get("prefix", prefix)
    prompt1 = prompt1 or params.get("prompt1", "")
    prompt2 = prompt2 or params.get("prompt2", "")

    infer_seed = clip(infer_seed, -1, 2**32 - 1)
    infer_seed = int(infer_seed)

    if not disable_normalize:
        text = text_normalize(text)

    sample_rate, audio_data = generate_audio(
        text=text,
        temperature=temperature,
        top_P=top_p,
        top_K=top_k,
        spk=spk,
        infer_seed=infer_seed,
        use_decoder=use_decoder,
        prompt1=prompt1,
        prompt2=prompt2,
        prefix=prefix,
    )

    return sample_rate, audio_data


# æ ¹æ®æ¢è¡Œåˆ†å‰²ï¼Œå†æ ¹æ®ä¸­æ–‡å¥å·åˆ†å‰²
def simple_split_text(text: str) -> list[str]:
    text = text.strip()
    text = text.replace("ã€‚", "ã€‚\n")
    text = text.split("\n")
    text = [t.strip() for t in text]
    text = [t for t in text if t]
    # è¿‡æ»¤æ‰åªåŒ…å«ç¬¦å·çš„æ®µè½
    text = [t for t in text if t and not re.fullmatch(r"\W+", t)]
    return text


@torch.inference_mode()
def tts_generate(
    text,
    temperature,
    top_p,
    top_k,
    spk,
    infer_seed,
    use_decoder,
    prompt1,
    prompt2,
    prefix,
    style,
    disable_normalize=False,
    batch_size=8,
):
    try:
        batch_size = int(batch_size)
    except Exception:
        batch_size = 8

    # åªæ”¯æŒå•ä¸ª1000å­—ä»¥å†…çš„æ–‡æœ¬
    text = text.strip()[0:1000]

    if style == "*auto":
        style = None

    if isinstance(top_k, float):
        top_k = int(top_k)

    params = calc_spk_style(spk=spk, style=style)

    spk = params.get("spk", spk)
    infer_seed = infer_seed or params.get("seed", infer_seed)
    temperature = temperature or params.get("temperature", temperature)
    prefix = prefix or params.get("prefix", prefix)
    prompt1 = prompt1 or params.get("prompt1", "")
    prompt2 = prompt2 or params.get("prompt2", "")

    infer_seed = clip(infer_seed, -1, 2**32 - 1)
    infer_seed = int(infer_seed)

    if not disable_normalize:
        text = text_normalize(text)

    if batch_size == 1:
        sample_rate, audio_data = generate_audio(
            text=text,
            temperature=temperature,
            top_P=top_p,
            top_K=top_k,
            spk=spk,
            infer_seed=infer_seed,
            use_decoder=use_decoder,
            prompt1=prompt1,
            prompt2=prompt2,
            prefix=prefix,
        )

        return sample_rate, audio_data
    else:
        texts = simple_split_text(text)
        audio_data_batch = generate_audio_batch(
            texts=texts,
            temperature=temperature,
            top_P=top_p,
            top_K=top_k,
            spk=spk,
            infer_seed=infer_seed,
            use_decoder=use_decoder,
            prompt1=prompt1,
            prompt2=prompt2,
            prefix=prefix,
        )
        sample_rate = audio_data_batch[0][0]
        audio_data = np.concatenate([data for _, data in audio_data_batch])

        return sample_rate, audio_data


@torch.inference_mode()
def refine_text(text: str, prompt: str):
    text = text_normalize(text)
    return refiner.refine_text(text, prompt=prompt)


def read_local_readme():
    with open("README.md", "r", encoding="utf-8") as file:
        content = file.read()
        content = content[content.index("# ğŸ—£ï¸ ChatTTS-Forge") :]
        return content


# æ¼”ç¤ºç¤ºä¾‹æ–‡æœ¬
sample_texts = [
    {
        "text": "å¤§ğŸŒï¼Œä¸€æ¡å¤§ğŸŒï¼Œå˜¿ï¼Œä½ çš„æ„Ÿè§‰çœŸçš„å¾ˆå¥‡å¦™  [lbreak]",
    },
    {
        "text": "å¤©æ°”é¢„æŠ¥æ˜¾ç¤ºï¼Œä»Šå¤©ä¼šæœ‰å°é›¨ï¼Œè¯·å¤§å®¶å‡ºé—¨æ—¶è®°å¾—å¸¦ä¼ã€‚é™æ¸©çš„å¤©æ°”ä¹Ÿæé†’æˆ‘ä»¬è¦é€‚æ—¶æ·»è¡£ä¿æš– [lbreak]",
    },
    {
        "text": "å…¬å¸çš„å¹´åº¦æ€»ç»“ä¼šè®®å°†åœ¨ä¸‹å‘¨ä¸‰ä¸¾è¡Œï¼Œè¯·å„éƒ¨é—¨æå‰å‡†å¤‡å¥½ç›¸å…³ææ–™ï¼Œç¡®ä¿ä¼šè®®é¡ºåˆ©è¿›è¡Œ [lbreak]",
    },
    {
        "text": "ä»Šå¤©çš„åˆé¤èœå•åŒ…æ‹¬çƒ¤é¸¡ã€æ²™æ‹‰å’Œè”¬èœæ±¤ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®è‡ªå·±çš„å£å‘³é€‰æ‹©é€‚åˆçš„èœå“ [lbreak]",
    },
    {
        "text": "è¯·æ³¨æ„ï¼Œç”µæ¢¯å°†åœ¨ä¸‹åˆä¸¤ç‚¹è¿›è¡Œä¾‹è¡Œç»´æŠ¤ï¼Œé¢„è®¡éœ€è¦ä¸€ä¸ªå°æ—¶çš„æ—¶é—´ï¼Œè¯·å¤§å®¶åœ¨æ­¤æœŸé—´ä½¿ç”¨æ¥¼æ¢¯ [lbreak]",
    },
    {
        "text": "å›¾ä¹¦é¦†æ–°åˆ°äº†ä¸€æ‰¹ä¹¦ç±ï¼Œæ¶µç›–äº†æ–‡å­¦ã€ç§‘å­¦å’Œå†å²ç­‰å¤šä¸ªé¢†åŸŸï¼Œæ¬¢è¿å¤§å®¶å‰æ¥å€Ÿé˜… [lbreak]",
    },
    {
        "text": "ç”µå½±ä¸­æ¢æœä¼Ÿæ‰®æ¼”çš„é™ˆæ°¸ä»çš„ç¼–å·27149 [lbreak]",
    },
    {
        "text": "è¿™å—é»„é‡‘é‡è¾¾324.75å…‹ [lbreak]",
    },
    {
        "text": "æˆ‘ä»¬ç­çš„æœ€é«˜æ€»åˆ†ä¸º583åˆ† [lbreak]",
    },
    {
        "text": "12~23 [lbreak]",
    },
    {
        "text": "-1.5~2 [lbreak]",
    },
    {
        "text": "å¥¹å‡ºç”Ÿäº86å¹´8æœˆ18æ—¥ï¼Œå¥¹å¼Ÿå¼Ÿå‡ºç”Ÿäº1995å¹´3æœˆ1æ—¥ [lbreak]",
    },
    {
        "text": "ç­‰ä¼šè¯·åœ¨12:05è¯·é€šçŸ¥æˆ‘ [lbreak]",
    },
    {
        "text": "ä»Šå¤©çš„æœ€ä½æ°”æ¸©è¾¾åˆ°-10Â°C [lbreak]",
    },
    {
        "text": "ç°åœºæœ‰7/12çš„è§‚ä¼—æŠ•å‡ºäº†èµæˆç¥¨ [lbreak]",
    },
    {
        "text": "æ˜å¤©æœ‰62ï¼…çš„æ¦‚ç‡é™é›¨ [lbreak]",
    },
    {
        "text": "éšä¾¿æ¥å‡ ä¸ªä»·æ ¼12å—5ï¼Œ34.5å…ƒï¼Œ20.1ä¸‡ [lbreak]",
    },
    {
        "text": "è¿™æ˜¯å›ºè¯0421-33441122 [lbreak]",
    },
    {
        "text": "è¿™æ˜¯æ‰‹æœº+86 18544139121 [lbreak]",
    },
]

ssml_example1 = """
<speak version="0.1">
    <voice spk="Bob" seed="42" style="narration-relaxed">
        ä¸‹é¢æ˜¯ä¸€ä¸ª ChatTTS ç”¨äºåˆæˆå¤šè§’è‰²å¤šæƒ…æ„Ÿçš„æœ‰å£°ä¹¦ç¤ºä¾‹[lbreak]
    </voice>
    <voice spk="Bob" seed="42" style="narration-relaxed">
        é»›ç‰å†·ç¬‘é“ï¼š[lbreak]
    </voice>
    <voice spk="female2" seed="42" style="angry">
        æˆ‘è¯´å‘¢ [uv_break] ï¼Œäºäº†ç»Šä½ï¼Œä¸ç„¶ï¼Œæ—©å°±é£èµ·æ¥äº†[lbreak]
    </voice>
    <voice spk="Bob" seed="42" style="narration-relaxed">
        å®ç‰é“ï¼š[lbreak]
    </voice>
    <voice spk="Alice" seed="42" style="unfriendly">
        â€œåªè®¸å’Œä½ ç© [uv_break] ï¼Œæ›¿ä½ è§£é—·ã€‚ä¸è¿‡å¶ç„¶åˆ°ä»–é‚£é‡Œï¼Œå°±è¯´è¿™äº›é—²è¯ã€‚â€[lbreak]
    </voice>
    <voice spk="female2" seed="42" style="angry">
        â€œå¥½æ²¡æ„æ€çš„è¯ï¼[uv_break] å»ä¸å»ï¼Œå…³æˆ‘ä»€ä¹ˆäº‹å„¿ï¼Ÿ åˆæ²¡å«ä½ æ›¿æˆ‘è§£é—·å„¿ [uv_break]ï¼Œè¿˜è®¸ä½ ä¸ç†æˆ‘å‘¢â€ [lbreak]
    </voice>
    <voice spk="Bob" seed="42" style="narration-relaxed">
        è¯´ç€ï¼Œä¾¿èµŒæ°”å›æˆ¿å»äº† [lbreak]
    </voice>
</speak>
"""
ssml_example2 = """
<speak version="0.1">
    <voice spk="Bob" seed="42" style="narration-relaxed">
        ä½¿ç”¨ prosody æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„è¯­é€Ÿè¯­è°ƒå’ŒéŸ³é‡ï¼Œç¤ºä¾‹å¦‚ä¸‹ [lbreak]

        <prosody>
            æ— ä»»ä½•é™åˆ¶å°†ä¼šç»§æ‰¿çˆ¶çº§voiceé…ç½®è¿›è¡Œç”Ÿæˆ [lbreak]
        </prosody>
        <prosody rate="1.5">
            è®¾ç½® rate å¤§äº1è¡¨ç¤ºåŠ é€Ÿï¼Œå°äº1ä¸ºå‡é€Ÿ [lbreak]
        </prosody>
        <prosody pitch="6">
            è®¾ç½® pitch è°ƒæ•´éŸ³è°ƒï¼Œè®¾ç½®ä¸º6è¡¨ç¤ºæé«˜6ä¸ªåŠéŸ³ [lbreak]
        </prosody>
        <prosody volume="2">
            è®¾ç½® volume è°ƒæ•´éŸ³é‡ï¼Œè®¾ç½®ä¸º2è¡¨ç¤ºæé«˜2ä¸ªåˆ†è´ [lbreak]
        </prosody>

        åœ¨ voice ä¸­æ— prosodyåŒ…è£¹çš„æ–‡æœ¬å³ä¸ºé»˜è®¤ç”ŸæˆçŠ¶æ€ä¸‹çš„è¯­éŸ³ [lbreak]
    </voice>
</speak>
"""
ssml_example3 = """
<speak version="0.1">
    <voice spk="Bob" seed="42" style="narration-relaxed">
        ä½¿ç”¨ break æ ‡ç­¾å°†ä¼šç®€å•çš„ [lbreak]
        
        <break time="500" />

        æ’å…¥ä¸€æ®µç©ºç™½åˆ°ç”Ÿæˆç»“æœä¸­ [lbreak]
    </voice>
</speak>
"""

ssml_example4 = """
<speak version="0.1">
    <voice spk="Bob" seed="42" style="excited">
        temperature for sampling (may be overridden by style or speaker) [lbreak]
        <break time="500" />
        æ¸©åº¦å€¼ç”¨äºé‡‡æ ·ï¼Œè¿™ä¸ªå€¼æœ‰å¯èƒ½è¢« style æˆ–è€… speaker è¦†ç›–  [lbreak]
        <break time="500" />
        temperature for sampling ï¼Œè¿™ä¸ªå€¼æœ‰å¯èƒ½è¢« style æˆ–è€… speaker è¦†ç›–  [lbreak]
        <break time="500" />
        æ¸©åº¦å€¼ç”¨äºé‡‡æ ·ï¼Œ(may be overridden by style or speaker) [lbreak]
    </voice>
</speak>
"""

default_ssml = """
<speak version="0.1">
  <voice spk="Bob" seed="42" style="narration-relaxed">
    è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„ SSML ç¤ºä¾‹ [lbreak] 
  </voice>
</speak>
"""


def create_tts_interface():
    speakers = get_speakers()

    def get_speaker_show_name(spk):
        if spk.gender == "*" or spk.gender == "":
            return spk.name
        return f"{spk.gender} : {spk.name}"

    speaker_names = ["*random"] + [
        get_speaker_show_name(speaker) for speaker in speakers
    ]

    styles = ["*auto"] + [s.get("name") for s in get_styles()]

    history = []

    with gr.Row():
        with gr.Column(scale=1):
            with gr.Group():
                gr.Markdown("ğŸ›ï¸Sampling")
                temperature_input = gr.Slider(
                    0.01, 2.0, value=0.3, step=0.01, label="Temperature"
                )
                top_p_input = gr.Slider(0.1, 1.0, value=0.7, step=0.1, label="Top P")
                top_k_input = gr.Slider(1, 50, value=20, step=1, label="Top K")
                batch_size_input = gr.Slider(1, 32, value=8, step=1, label="Batch Size")

            with gr.Row():
                with gr.Group():
                    gr.Markdown("ğŸ­Style")
                    gr.Markdown("- åç¼€ä¸º `_p` è¡¨ç¤ºå¸¦promptï¼Œæ•ˆæœæ›´å¼ºä½†æ˜¯å½±å“è´¨é‡")
                    style_input_dropdown = gr.Dropdown(
                        choices=styles,
                        # label="Choose Style",
                        interactive=True,
                        show_label=False,
                        value="*auto",
                    )
            with gr.Row():
                with gr.Group():
                    gr.Markdown("ğŸ—£ï¸Speaker (Name or Seed)")
                    spk_input_text = gr.Textbox(
                        label="Speaker (Text or Seed)",
                        value="female2",
                        show_label=False,
                    )
                    spk_input_dropdown = gr.Dropdown(
                        choices=speaker_names,
                        # label="Choose Speaker",
                        interactive=True,
                        value="female2",
                        show_label=False,
                    )
                    spk_rand_button = gr.Button(
                        value="ğŸ²",
                        # tooltip="Random Seed",
                        variant="secondary",
                    )
                    spk_input_dropdown.change(
                        fn=lambda x: x.startswith("*")
                        and "-1"
                        or x.split(":")[-1].strip(),
                        inputs=[spk_input_dropdown],
                        outputs=[spk_input_text],
                    )
                    spk_rand_button.click(
                        lambda x: str(torch.randint(0, 2**32 - 1, (1,)).item()),
                        inputs=[spk_input_text],
                        outputs=[spk_input_text],
                    )
            with gr.Group():
                gr.Markdown("ğŸ’ƒInference Seed")
                infer_seed_input = gr.Number(
                    value=42,
                    label="Inference Seed",
                    show_label=False,
                    minimum=-1,
                    maximum=2**32 - 1,
                )
                infer_seed_rand_button = gr.Button(
                    value="ğŸ²",
                    # tooltip="Random Seed",
                    variant="secondary",
                )
            use_decoder_input = gr.Checkbox(
                value=True, label="Use Decoder", visible=False
            )
            with gr.Group():
                gr.Markdown("ğŸ”§Prompt engineering")
                prompt1_input = gr.Textbox(label="Prompt 1")
                prompt2_input = gr.Textbox(label="Prompt 2")
                prefix_input = gr.Textbox(label="Prefix")

            infer_seed_rand_button.click(
                lambda x: int(torch.randint(0, 2**32 - 1, (1,)).item()),
                inputs=[infer_seed_input],
                outputs=[infer_seed_input],
            )
        with gr.Column(scale=3):
            with gr.Row():
                with gr.Column(scale=4):
                    with gr.Group():
                        input_title = gr.Markdown(
                            "ğŸ“Text Input",
                            elem_id="input-title",
                        )
                        gr.Markdown("- æ¯ä¸ªbatchæœ€é•¿30s")
                        gr.Markdown("- batch sizeè®¾ç½®ä¸º1ï¼Œå³ä¸ä½¿ç”¨æ‰¹å¤„ç†")
                        gr.Markdown("- å¼€å¯batchè¯·é…åˆè®¾ç½®Inference Seed")
                        gr.Markdown("- å­—æ•°é™åˆ¶1,000å­—ï¼Œè¶…è¿‡éƒ¨åˆ†æˆªæ–­")
                        gr.Markdown("- å¦‚æœå°¾å­—åå­—ä¸è¯»ï¼Œå¯ä»¥è¯•è¯•ç»“å°¾åŠ ä¸Š `[lbreak]`")
                        text_input = gr.Textbox(
                            show_label=False,
                            label="Text to Speech",
                            lines=10,
                            placeholder="è¾“å…¥æ–‡æœ¬æˆ–é€‰æ‹©ç¤ºä¾‹",
                            elem_id="text-input",
                        )
                        # TODO å­—æ•°ç»Ÿè®¡ï¼Œå…¶å®å®ç°å¾ˆå¥½å†™ï¼Œä½†æ˜¯å°±æ˜¯ä¼šè§¦å‘loading...å¹¶ä¸”è¿˜è¦å’Œåç«¯äº¤äº’...
                        # text_input.change(
                        #     fn=lambda x: (
                        #         f"ğŸ“Text Input ({len(x)} char)"
                        #         if x
                        #         else (
                        #             "ğŸ“Text Input (0 char)"
                        #             if not x
                        #             else "ğŸ“Text Input (0 char)"
                        #         )
                        #     ),
                        #     inputs=[text_input],
                        #     outputs=[input_title],
                        # )
                        with gr.Row():
                            contorl_tokens = [
                                "[laugh]",
                                "[uv_break]",
                                "[v_break]",
                                "[lbreak]",
                            ]

                            for tk in contorl_tokens:
                                t_btn = gr.Button(tk)
                                t_btn.click(
                                    lambda text, tk=tk: text + " " + tk,
                                    inputs=[text_input],
                                    outputs=[text_input],
                                )
                with gr.Column(scale=1):
                    with gr.Group():
                        gr.Markdown("ğŸ¶Refiner")
                        refine_prompt_input = gr.Textbox(
                            label="Refine Prompt",
                            value="[oral_2][laugh_0][break_6]",
                        )
                        refine_button = gr.Button("âœï¸Refine Text")
                        # TODO åˆ†å‰²å¥å­ï¼Œä½¿ç”¨å½“å‰é…ç½®æ‹¼æ¥ä¸ºSSMLï¼Œç„¶åå‘é€åˆ°SSML tab
                        # send_button = gr.Button("ğŸ“©Split and send to SSML")

                    with gr.Group():
                        gr.Markdown("ğŸ”ŠGenerate")
                        disable_normalize_input = gr.Checkbox(
                            value=False, label="Disable Normalize"
                        )
                        tts_button = gr.Button(
                            "ğŸ”ŠGenerate Audio",
                            variant="primary",
                            elem_classes="big-button",
                        )

            with gr.Group():
                gr.Markdown("ğŸ„Examples")
                sample_dropdown = gr.Dropdown(
                    choices=[sample["text"] for sample in sample_texts],
                    show_label=False,
                    value=None,
                    interactive=True,
                )
                sample_dropdown.change(
                    fn=lambda x: x,
                    inputs=[sample_dropdown],
                    outputs=[text_input],
                )

            with gr.Group():
                gr.Markdown("ğŸ¨Output")
                tts_output = gr.Audio(label="Generated Audio")

    refine_button.click(
        refine_text,
        inputs=[text_input, refine_prompt_input],
        outputs=[text_input],
    )

    tts_button.click(
        tts_generate,
        inputs=[
            text_input,
            temperature_input,
            top_p_input,
            top_k_input,
            spk_input_text,
            infer_seed_input,
            use_decoder_input,
            prompt1_input,
            prompt2_input,
            prefix_input,
            style_input_dropdown,
            disable_normalize_input,
            batch_size_input,
        ],
        outputs=tts_output,
    )


def create_ssml_interface():
    examples = [
        ssml_example1,
        ssml_example2,
        ssml_example3,
        ssml_example4,
    ]

    with gr.Row():
        with gr.Column(scale=3):
            with gr.Group():
                gr.Markdown("ğŸ“SSML Input")
                gr.Markdown("- æœ€é•¿2000å­—ç¬¦ï¼Œè¶…è¿‡ä¼šè¢«æˆªæ–­")
                gr.Markdown("- å°½é‡ä¿è¯ä½¿ç”¨ç›¸åŒçš„ seed")
                gr.Markdown(
                    "- å…³äºSSMLå¯ä»¥çœ‹è¿™ä¸ª [æ–‡æ¡£](https://github.com/lenML/ChatTTS-Forge/blob/main/docs/SSML.md)"
                )
                ssml_input = gr.Textbox(
                    label="SSML Input",
                    lines=10,
                    value=default_ssml,
                    placeholder="è¾“å…¥ SSML æˆ–é€‰æ‹©ç¤ºä¾‹",
                    elem_id="ssml_input",
                    show_label=False,
                )
                ssml_button = gr.Button("ğŸ”ŠSynthesize SSML", variant="primary")
        with gr.Column(scale=1):
            with gr.Group():
                # å‚æ•°
                gr.Markdown("ğŸ›ï¸Parameters")
                # batch size
                batch_size_input = gr.Number(
                    label="Batch Size",
                    value=8,
                    minimum=1,
                    maximum=32,
                    step=1,
                )
            with gr.Group():
                gr.Markdown("ğŸ„Examples")
                gr.Examples(
                    examples=examples,
                    inputs=[ssml_input],
                )

    ssml_output = gr.Audio(label="Generated Audio")

    ssml_button.click(
        synthesize_ssml,
        inputs=[ssml_input, batch_size_input],
        outputs=ssml_output,
    )

    return ssml_input


def split_long_text(long_text_input):
    long_text = simple_split_text(long_text_input)
    data = []
    for i, text in enumerate(long_text):
        data.append([i, text, len(text)])
    return data


def merge_dataframe_to_ssml(dataframe, spk, style, seed):
    if style == "*auto":
        style = None
    if spk == "-1" or spk == -1:
        spk = None
    if seed == -1 or seed == "-1":
        seed = None

    ssml = ""
    indent = " " * 2

    for i, row in dataframe.iterrows():
        ssml += f"{indent}<voice"
        if spk:
            ssml += f' spk="{spk}"'
        if style:
            ssml += f' style="{style}"'
        if seed:
            ssml += f' seed="{seed}"'
        ssml += ">\n"
        ssml += f"{indent}{indent}{row[1]}\n"
        ssml += f"{indent}</voice>\n"
    return f"<speak version='0.1'>\n{ssml}</speak>"


# é•¿æ–‡æœ¬å¤„ç†
# å¯ä»¥è¾“å…¥é•¿æ–‡æœ¬ï¼Œå¹¶é€‰æ‹©åˆ‡å‰²æ–¹æ³•ï¼Œåˆ‡å‰²ä¹‹åå¯ä»¥å°†æ‹¼æ¥çš„SSMLå‘é€åˆ°SSML tab
# æ ¹æ® ã€‚ å¥å·åˆ‡å‰²ï¼Œåˆ‡å‰²ä¹‹åæ˜¾ç¤ºåˆ° data table
def create_long_content_tab(ssml_input, tabs):
    speakers = get_speakers()

    def get_speaker_show_name(spk):
        if spk.gender == "*" or spk.gender == "":
            return spk.name
        return f"{spk.gender} : {spk.name}"

    speaker_names = ["*random"] + [
        get_speaker_show_name(speaker) for speaker in speakers
    ]

    styles = ["*auto"] + [s.get("name") for s in get_styles()]

    with gr.Row():
        with gr.Column(scale=3):
            with gr.Group():
                gr.Markdown("ğŸ“Long Text Input")
                gr.Markdown("- æ­¤é¡µé¢ç”¨äºå¤„ç†è¶…é•¿æ–‡æœ¬")
                gr.Markdown("- åˆ‡å‰²åï¼Œå¯ä»¥é€‰æ‹©è¯´è¯äººã€é£æ ¼ã€seedï¼Œç„¶åå‘é€åˆ°SSML")
                long_text_input = gr.Textbox(
                    label="Long Text Input",
                    lines=10,
                    placeholder="è¾“å…¥é•¿æ–‡æœ¬",
                    elem_id="long-text-input",
                    show_label=False,
                )
                long_text_split_button = gr.Button("ğŸ”ªSplit Text")

    with gr.Row():
        with gr.Column(scale=3):
            with gr.Group():
                gr.Markdown("ğŸ¨Output")
                long_text_output = gr.DataFrame(
                    headers=["index", "text", "length"],
                    datatype=["number", "str", "number"],
                    elem_id="long-text-output",
                    interactive=False,
                    wrap=True,
                    value=[],
                )
        with gr.Column(scale=1):
            # é€‰æ‹©è¯´è¯äºº é€‰æ‹©é£æ ¼ é€‰æ‹©seed
            with gr.Group():
                gr.Markdown("ğŸ—£ï¸Speaker")
                spk_input_text = gr.Textbox(
                    label="Speaker (Text or Seed)",
                    value="female2",
                    show_label=False,
                )
                spk_input_dropdown = gr.Dropdown(
                    choices=speaker_names,
                    interactive=True,
                    value="female2",
                    show_label=False,
                )
                spk_rand_button = gr.Button(
                    value="ğŸ²",
                    variant="secondary",
                )
                spk_input_dropdown.change(
                    fn=lambda x: x.startswith("*") and "-1" or x.split(":")[-1].strip(),
                    inputs=[spk_input_dropdown],
                    outputs=[spk_input_text],
                )
                spk_rand_button.click(
                    lambda x: int(torch.randint(0, 2**32 - 1, (1,)).item()),
                    inputs=[spk_input_text],
                    outputs=[spk_input_text],
                )
            with gr.Group():
                gr.Markdown("ğŸ­Style")
                style_input_dropdown = gr.Dropdown(
                    choices=styles,
                    interactive=True,
                    show_label=False,
                    value="*auto",
                )
            with gr.Group():
                gr.Markdown("ğŸ—£ï¸Seed")
                infer_seed_input = gr.Number(
                    value=42,
                    label="Inference Seed",
                    show_label=False,
                    minimum=-1,
                    maximum=2**32 - 1,
                )
                infer_seed_rand_button = gr.Button(
                    value="ğŸ²",
                    variant="secondary",
                )
                infer_seed_rand_button.click(
                    lambda x: int(torch.randint(0, 2**32 - 1, (1,)).item()),
                    inputs=[infer_seed_input],
                    outputs=[infer_seed_input],
                )

            send_btn = gr.Button("ğŸ“©Send to SSML", variant="primary")

            send_btn.click(
                merge_dataframe_to_ssml,
                inputs=[
                    long_text_output,
                    spk_input_text,
                    style_input_dropdown,
                    infer_seed_input,
                ],
                outputs=[ssml_input],
            )

            def change_tab():
                return gr.Tabs(selected="ssml")

            send_btn.click(change_tab, inputs=[], outputs=[tabs])

        long_text_split_button.click(
            split_long_text,
            inputs=[long_text_input],
            outputs=[long_text_output],
        )


def create_readme_tab():
    readme_content = read_local_readme()
    gr.Markdown(readme_content)


def create_interface():

    js_func = """
    function refresh() {
        const url = new URL(window.location);

        if (url.searchParams.get('__theme') !== 'dark') {
            url.searchParams.set('__theme', 'dark');
            window.location.href = url.href;
        }
    }
    """

    head_js = """
    <script>
    </script>
    """

    with gr.Blocks(js=js_func, head=head_js, title="ChatTTS Forge WebUI") as demo:
        css = """
        <style>
        .big-button {
            height: 80px;
        }
        #input_title div.eta-bar {
            display: none !important; transform: none !important;
        }
        </style>
        """

        gr.HTML(css)
        with gr.Tabs() as tabs:
            with gr.TabItem("TTS"):
                create_tts_interface()

            with gr.TabItem("SSML", id="ssml"):
                ssml_input = create_ssml_interface()

            with gr.TabItem("Long Text"):
                create_long_content_tab(ssml_input, tabs=tabs)

            with gr.TabItem("README"):
                create_readme_tab()

        gr.Markdown(
            "æ­¤é¡¹ç›®åŸºäº [ChatTTS-Forge](https://github.com/lenML/ChatTTS-Forge) "
        )
    return demo


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Gradio App")
    parser.add_argument(
        "--server_name", type=str, default="0.0.0.0", help="server name"
    )
    parser.add_argument("--server_port", type=int, default=7860, help="server port")
    parser.add_argument(
        "--share", action="store_true", help="share the gradio interface"
    )
    parser.add_argument("--debug", action="store_true", help="enable debug mode")
    parser.add_argument("--auth", type=str, help="username:password for authentication")
    parser.add_argument(
        "--half",
        action="store_true",
        help="Enable half precision for model inference",
    )
    parser.add_argument(
        "--off_tqdm",
        action="store_true",
        help="Disable tqdm progress bar",
    )

    args = parser.parse_args()

    server_name = os.getenv("GRADIO_SERVER_NAME", args.server_name)
    server_port = int(os.getenv("GRADIO_SERVER_PORT", args.server_port))
    share = bool(os.getenv("GRADIO_SHARE", args.share))
    debug = bool(os.getenv("GRADIO_DEBUG", args.debug))
    auth = os.getenv("GRADIO_AUTH", args.auth)
    half = bool(os.getenv("MODEL_HALF", args.half))
    off_tqdm = bool(os.getenv("DISABLE_TQDM", args.off_tqdm))

    demo = create_interface()

    if auth:
        auth = tuple(auth.split(":"))

    if half:
        config.model_config["half"] = True

    if off_tqdm:
        config.disable_tqdm = True

    demo.queue().launch(
        server_name=server_name,
        server_port=server_port,
        share=share,
        debug=debug,
        auth=auth,
        show_api=False,
    )
